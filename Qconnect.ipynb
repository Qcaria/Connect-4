{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "weAJPVOp7IH3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qr9xd8ch7NAf"
   },
   "outputs": [],
   "source": [
    "class Connect4:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((6, 7), dtype=np.int8)\n",
    "        self.current_player = 1\n",
    "        \n",
    "    def make_move(self, col):\n",
    "        if not self.is_valid_move(col):\n",
    "            self.current_player = -1 if self.current_player == 1 else 1\n",
    "            return -20, False  # Return a strong punishment\n",
    "\n",
    "        # Make the move and check if it results in a win\n",
    "        row = self.get_open_row(col)\n",
    "        self.board[row][col] = self.current_player\n",
    "        winner = self.check_winner()\n",
    "        self.current_player = -1 if self.current_player == 1 else 1\n",
    "\n",
    "        if winner == self.current_player:\n",
    "            raise Exception(\"Impossible stuff\")\n",
    "        elif winner == 0:\n",
    "            if self.board.all():\n",
    "                return 0, True\n",
    "            else:\n",
    "                return 0, False\n",
    "        else:\n",
    "            # The current player won the game, so return a large positive reward\n",
    "            return 10, True\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        if col < 0 or col >= 7:\n",
    "            return False\n",
    "\n",
    "        if self.board[0][col] != 0:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_open_row(self, col):\n",
    "        rows = np.where(self.board[:,col] == 0)[0]\n",
    "        if len(rows) == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return rows[-1]\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.reshape(self.board, (1, 42))\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check horizontal\n",
    "        for r in range(6):\n",
    "            row = self.board[r,:]\n",
    "            for c in range(4):\n",
    "                if row[c] != 0 and np.all(row[c] == row[c+1:c+4]):\n",
    "                    return row[c]\n",
    "\n",
    "        # Check vertical\n",
    "        for c in range(7):\n",
    "            col = self.board[:,c]\n",
    "            for r in range(3):\n",
    "                if col[r] != 0 and np.all(col[r] == col[r+1:r+4]):\n",
    "                    return col[r]\n",
    "\n",
    "        # Check diagonal\n",
    "        for r in range(3):\n",
    "            for c in range(4):\n",
    "                if self.board[r][c] != 0 and np.all(np.array([self.board[r+i][c+i] for i in range(4)]) == self.board[r][c]):\n",
    "                    return self.board[r][c]\n",
    "\n",
    "        # Check other diagonal\n",
    "        for r in range(3):\n",
    "            for c in range(4):\n",
    "                if self.board[r][c+3] != 0 and np.all(np.array([self.board[r+i][c+3-i] for i in range(4)]) == self.board[r][c+3]):\n",
    "                    return self.board[r][c+3]\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def is_game_over(self):\n",
    "        if self.check_winner() != 0:\n",
    "            return True\n",
    "\n",
    "        return np.all(self.board != 0)\n",
    "\n",
    "    def print_board(self):\n",
    "        print(\"Connect 4\")\n",
    "        print(\"-----------------\")\n",
    "        for r in range(6):\n",
    "            row = \"\"\n",
    "            for c in range(7):\n",
    "                if self.board[r][c] == 0:\n",
    "                    row += \" -\"\n",
    "                elif self.board[r][c] == 1:\n",
    "                    row += \" X\"\n",
    "                else:\n",
    "                    row += \" O\"\n",
    "            print(row)\n",
    "        print(\"-----------------\")\n",
    "        print(\" 0 1 2 3 4 5 6\")\n",
    "        print()\n",
    "               \n",
    "    def play_game(self):\n",
    "        while not self.is_game_over():\n",
    "            self.print_board()\n",
    "\n",
    "            col = int(input(\"Player %d: Enter a column (0-6) to place your piece: \" % self.current_player))\n",
    "            print()\n",
    "            if not self.make_move(col):\n",
    "                print(\"Invalid move. Please try again.\\n\")\n",
    "            \n",
    "\n",
    "        self.print_board()\n",
    "\n",
    "        winner = self.check_winner()\n",
    "        if winner == 0:\n",
    "            print(\"The game ended in a tie.\")\n",
    "        else:\n",
    "            print(\"Player %d wins!\" % winner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xH0eBkj27Qgl"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vdTqqL_q7S4Y"
   },
   "outputs": [],
   "source": [
    "class Human():\n",
    "    def choose_move(self, state, eps):\n",
    "        col = int(input(\"Elige tu jugada: \"))\n",
    "        return col\n",
    "\n",
    "class RNDAgent():\n",
    "    \n",
    "    def choose_move(self, state, eps):\n",
    "        return torch.randint(7, (1,), device = device)\n",
    "\n",
    "class DQNAgent():\n",
    "    \n",
    "    def __init__(self, policy, target):\n",
    "        self.memo = ReplayMemory(MAX_MEMO)\n",
    "        self.policy = policy\n",
    "        self.target = target\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def choose_move(self, state, eps):\n",
    "        self.policy.eval()\n",
    "        if random.random() < eps:\n",
    "            return torch.randint(7, (1,), device = device)\n",
    "        else:\n",
    "            return self.policy(state).max(1)[1]\n",
    "    \n",
    "    def replay(self): \n",
    "        states, actions, next_states, rewards = zip(*self.memo.sample(BATCH_SIZE))\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.stack(rewards)\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next_states)), device = device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in next_states if s is not None])\n",
    "        \n",
    "        self.policy.train()\n",
    "        pred_Q = self.policy(states).gather(1, actions)\n",
    "        self.target.eval()\n",
    "        max_next_Q = torch.zeros(BATCH_SIZE, 1, device = device)\n",
    "        with torch.no_grad():\n",
    "            max_next_Q[non_final_mask] = self.target(non_final_next_states).max(1, keepdims = True)[0]\n",
    "\n",
    "        target_Q = rewards - GAMMA * max_next_Q\n",
    "\n",
    "        loss = F.huber_loss(pred_Q, target_Q)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, 4, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(128, 128, 2, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*3*128, 512, bias=False),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 7),\n",
    "        )\n",
    "        \n",
    "    def forward(self, xin):\n",
    "        B, T = xin.shape\n",
    "        x = xin.view(B, 1, 6, 7)\n",
    "        return self.net(x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def battle(agent1, agent2, render = False):\n",
    "    current_player = agent1\n",
    "    game = Connect4()\n",
    "    state = torch.from_numpy(game.get_state() * game.current_player).float().to(device)\n",
    "    turn = 0\n",
    "    if render: game.print_board()\n",
    "        \n",
    "    while True:\n",
    "        action = current_player.choose_move(state, 0.0)\n",
    "        reward, finished = game.make_move(action)\n",
    "        \n",
    "        if render:\n",
    "            game.print_board()\n",
    "            \n",
    "        if finished:\n",
    "            if reward == 0:\n",
    "                return 0\n",
    "            return -game.current_player\n",
    "        \n",
    "        state = torch.from_numpy(game.get_state() * game.current_player).float().to(device)\n",
    "        current_player = agent2 if current_player == agent1 else agent1\n",
    "        \n",
    "        turn += 1\n",
    "        if turn >= MAX_TURNS:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZG0Bvzb47Um1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 25000\n",
    "TAU = 0.001\n",
    "\n",
    "GAMMA = 0.95\n",
    "MAX_MEMO = 100000\n",
    "MIN_MEMO = 1000\n",
    "MAX_ITERS = 300000\n",
    "\n",
    "BATTLE_FREQ = 3500\n",
    "BATTLE_NUM = 1000\n",
    "MAX_TURNS = 100\n",
    "\n",
    "policy_net = Network().to(device)\n",
    "policy_net.load_state_dict(torch.load('net.pth', map_location=device))\n",
    "\n",
    "target_net = Network().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "agent = DQNAgent(policy_net, target_net)\n",
    "rnd_agent = RNDAgent()\n",
    "player = Human()\n",
    "\n",
    "record = 90\n",
    "loss_record = 0.5\n",
    "record_holder = policy_net.state_dict()\n",
    "loss_mem = []\n",
    "wrate_mem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOnixkGL7XK5",
    "outputId": "c5c0b147-504e-4bee-d9a8-40bca7eadafe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Step 0 | epsilon 0.60\n",
      "Player 1: 8 | Player 2: 992 | wrate: 99.2%\n",
      "Learning Rate: 3.5e-05\n",
      "Mean loss: 0.055575\n",
      "-------------------------------\n",
      "Step 3500 | epsilon 0.52\n",
      "Player 1: 15 | Player 2: 985 | wrate: 98.5%\n",
      "Learning Rate: 3.5e-05\n",
      "Mean loss: 0.060350\n",
      "-------------------------------\n",
      "Step 7000 | epsilon 0.45\n",
      "Player 1: 8 | Player 2: 992 | wrate: 99.2%\n",
      "Learning Rate: 3.5e-05\n",
      "Mean loss: 0.056073\n",
      "-------------------------------\n",
      "Step 10500 | epsilon 0.40\n",
      "Player 1: 15 | Player 2: 985 | wrate: 98.5%\n",
      "Learning Rate: 3.5e-05\n",
      "Mean loss: 0.054917\n",
      "-------------------------------\n",
      "Step 14000 | epsilon 0.35\n",
      "Player 1: 12 | Player 2: 988 | wrate: 98.8%\n",
      "Learning Rate: 3.5e-05\n",
      "Mean loss: 0.054472\n",
      "-------------------------------\n",
      "Step 17500 | epsilon 0.31\n",
      "Player 1: 14 | Player 2: 986 | wrate: 98.6%\n",
      "Learning Rate: 3.5e-05\n",
      "Mean loss: 0.054948\n",
      "-------------------------------\n",
      "Step 21000 | epsilon 0.27\n",
      "Player 1: 23 | Player 2: 977 | wrate: 97.7%\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "Mean loss: 0.052119\n",
      "-------------------------------\n",
      "Step 24500 | epsilon 0.25\n",
      "Player 1: 15 | Player 2: 985 | wrate: 98.5%\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "Mean loss: 0.053703\n",
      "-------------------------------\n",
      "Step 28000 | epsilon 0.22\n",
      "Player 1: 16 | Player 2: 984 | wrate: 98.4%\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "Mean loss: 0.057901\n",
      "-------------------------------\n",
      "Step 31500 | epsilon 0.20\n",
      "Player 1: 18 | Player 2: 982 | wrate: 98.2%\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "Mean loss: 0.058374\n",
      "-------------------------------\n",
      "Step 35000 | epsilon 0.19\n",
      "Player 1: 16 | Player 2: 984 | wrate: 98.4%\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "Mean loss: 0.057930\n",
      "-------------------------------\n",
      "Step 38500 | epsilon 0.17\n",
      "Player 1: 16 | Player 2: 984 | wrate: 98.4%\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "Mean loss: 0.057107\n",
      "-------------------------------\n",
      "Step 42000 | epsilon 0.16\n",
      "Player 1: 22 | Player 2: 978 | wrate: 97.8%\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "Mean loss: 0.055589\n",
      "-------------------------------\n",
      "Step 45500 | epsilon 0.15\n",
      "Player 1: 14 | Player 2: 986 | wrate: 98.6%\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "Mean loss: 0.056778\n",
      "-------------------------------\n",
      "Step 49000 | epsilon 0.14\n",
      "Player 1: 17 | Player 2: 983 | wrate: 98.3%\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "Mean loss: 0.058334\n",
      "-------------------------------\n",
      "Step 52500 | epsilon 0.14\n",
      "Player 1: 21 | Player 2: 979 | wrate: 97.9%\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "Mean loss: 0.060467\n",
      "-------------------------------\n",
      "Step 56000 | epsilon 0.13\n",
      "Player 1: 20 | Player 2: 980 | wrate: 98.0%\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "Mean loss: 0.061965\n",
      "-------------------------------\n",
      "Step 59500 | epsilon 0.13\n",
      "Player 1: 15 | Player 2: 985 | wrate: 98.5%\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "Mean loss: 0.061931\n",
      "-------------------------------\n",
      "Step 63000 | epsilon 0.12\n",
      "Player 1: 19 | Player 2: 981 | wrate: 98.1%\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "Mean loss: 0.064299\n",
      "-------------------------------\n",
      "Step 66500 | epsilon 0.12\n",
      "Player 1: 19 | Player 2: 981 | wrate: 98.1%\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "Mean loss: 0.067158\n",
      "-------------------------------\n",
      "Step 70000 | epsilon 0.12\n",
      "Player 1: 11 | Player 2: 989 | wrate: 98.9%\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "Mean loss: 0.071439\n",
      "-------------------------------\n",
      "Step 73500 | epsilon 0.11\n",
      "Player 1: 16 | Player 2: 984 | wrate: 98.4%\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "Mean loss: 0.076394\n",
      "-------------------------------\n",
      "Step 77000 | epsilon 0.11\n",
      "Player 1: 18 | Player 2: 982 | wrate: 98.2%\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "Mean loss: 0.076646\n",
      "-------------------------------\n",
      "Step 80500 | epsilon 0.11\n",
      "Player 1: 11 | Player 2: 989 | wrate: 98.9%\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "Mean loss: 0.079639\n",
      "-------------------------------\n",
      "Step 84000 | epsilon 0.11\n",
      "Player 1: 19 | Player 2: 981 | wrate: 98.1%\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "Mean loss: 0.084873\n",
      "-------------------------------\n",
      "Step 87500 | epsilon 0.11\n",
      "Player 1: 18 | Player 2: 982 | wrate: 98.2%\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "Mean loss: 0.091635\n",
      "-------------------------------\n",
      "Step 91000 | epsilon 0.11\n",
      "Player 1: 19 | Player 2: 981 | wrate: 98.1%\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "Mean loss: 0.093336\n",
      "-------------------------------\n",
      "Step 94500 | epsilon 0.10\n",
      "Player 1: 14 | Player 2: 986 | wrate: 98.6%\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "Mean loss: 0.096667\n",
      "-------------------------------\n",
      "Step 98000 | epsilon 0.10\n",
      "Player 1: 13 | Player 2: 987 | wrate: 98.7%\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "Mean loss: 0.100129\n"
     ]
    }
   ],
   "source": [
    "game = Connect4()\n",
    "state = torch.from_numpy(game.get_state() * game.current_player).float().to(device)\n",
    "\n",
    "for step in range(MAX_ITERS):\n",
    "    eps = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * step / EPS_DECAY)\n",
    "\n",
    "    if step % BATTLE_FREQ == 0:\n",
    "        w1 = 0\n",
    "        w2 = 0\n",
    "        for _ in range(BATTLE_NUM):\n",
    "            winner = battle(rnd_agent, agent)\n",
    "            w1 += winner == 1\n",
    "            w2 += winner == -1\n",
    "        wrate = (w2 / BATTLE_NUM)*100\n",
    "        wrate_mem.append(wrate)\n",
    "        print('-------------------------------')\n",
    "        print(f'Step {step} | epsilon {eps:.2f}')\n",
    "        print(f'Player 1: {w1} | Player 2: {w2} | wrate: {wrate:.1f}%')\n",
    "\n",
    "        if len(loss_mem) >= BATTLE_FREQ:\n",
    "            meanloss = sum(loss_mem[-BATTLE_FREQ:])/BATTLE_FREQ\n",
    "            print(f'Mean loss: {meanloss:.6f}')\n",
    "\n",
    "            if wrate == record and meanloss < loss_record:\n",
    "                loss_record = meanloss\n",
    "                record_holder = policy_net.state_dict()\n",
    "                torch.save(record_holder, 'net.pth')\n",
    "                print(f'The champion got {loss_record} mean loss')\n",
    "            elif wrate > record:\n",
    "                loss_record = meanloss    \n",
    "                record = wrate\n",
    "                record_holder = policy_net.state_dict()\n",
    "                torch.save(record_holder, 'net.pth')\n",
    "                print(f'The champion got {record} winrate')\n",
    "\n",
    "    #play the game\n",
    "    action = agent.choose_move(state, eps)\n",
    "    reward, finished = game.make_move(action)\n",
    "    reward = torch.tensor([reward], device = device)\n",
    "    \n",
    "    if finished:\n",
    "        next_state = None\n",
    "        game.reset()\n",
    "    else:  \n",
    "        next_state = torch.from_numpy(game.get_state() * game.current_player).float().to(device)\n",
    "    \n",
    "    #remember\n",
    "    agent.memo.push(state, action, next_state, reward)\n",
    "\n",
    "    #replay\n",
    "    if len(agent.memo) >= MIN_MEMO:\n",
    "        #Train policy\n",
    "        loss = agent.replay()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Train target\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        loss_mem.append(loss.item())\n",
    "    \n",
    "    #prepare next iter\n",
    "    if finished:\n",
    "        state = torch.from_numpy(game.get_state() * game.current_player).float().to(device)\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "szHUwCfvzhgT",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - X - - - - -\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 6\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - X - - - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - X - X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 2\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - X - - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 3\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - X O - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - X - - - -\n",
      " - - X O - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 2\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - - X - - - -\n",
      " - - X O - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - - X X - - -\n",
      " - - X O - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 1\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - - X X - - -\n",
      " - O X O - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - X X X - - -\n",
      " - O X O - - -\n",
      " - X O X - - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 4\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - X X X - - -\n",
      " - O X O - - -\n",
      " - X O X O - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - X X X - - -\n",
      " - O X O X - -\n",
      " - X O X O - O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Elige tu jugada: 5\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - X X X - - -\n",
      " - O X O X - -\n",
      " - X O X O O O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n",
      "Connect 4\n",
      "-----------------\n",
      " - - - - - - -\n",
      " - - - - - - -\n",
      " - - O - - - -\n",
      " - X X X X - -\n",
      " - O X O X - -\n",
      " - X O X O O O\n",
      "-----------------\n",
      " 0 1 2 3 4 5 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "battle(agent, player, render = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
